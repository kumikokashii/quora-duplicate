{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helper_functions import *\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pickle.load(open('quora_questions_train_features_v2.pickle', 'rb'))\n",
    "test = pd.DataFrame()\n",
    "for i in range(1, 6):\n",
    "    test = pd.concat([test, pickle.load(open('quora_questions_test_features_v2_part' + str(i) + '.pickle', 'rb'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['cosine_sim', 'all_tokens_count', 'common_tokens_count', 'common_tokens_ratio', \n",
    "            'char_count_question1', 'char_count_question2', 'ratio_char_count',  \n",
    "            'token_count_question1', 'token_count_question2', 'ratio_token_count', \n",
    "            'common_tokens_count_question1', 'common_tokens_count_question2', \n",
    "            'common_tokens_ratio_question1', 'common_tokens_ratio_question2', \n",
    "            'ratio_common_tokens_ratio'] + list(range(0, 200))\n",
    "train_X = train[features]\n",
    "train_y = train['is_duplicate']\n",
    "test_X = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train into train (75%) and validation (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, validation_X, train_y, validation_y = train_test_split(train_X, train_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = train_X.as_matrix().astype(np.float32)\n",
    "validation_X = validation_X.as_matrix().astype(np.float32)\n",
    "test_X = test_X.as_matrix().astype(np.float32)\n",
    "\n",
    "train_y = np.reshape(train_y, [-1, 1]).astype(np.float32)\n",
    "validation_y = np.reshape(validation_y, [-1, 1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "naming = 'nn_01'\n",
    "#############################\n",
    "variables = {\n",
    "    'feature_size': len(features),\n",
    "    'first_hidden_layer_features': 512,\n",
    "    'second_hidden_layer_features': 128,\n",
    "    'third_hidden_layer_features': 64, \n",
    "    'weights_stddev': 0.015,\n",
    "    'biases_initial': 0.1,\n",
    "    'dropout_train_keep_prob': 0.5,\n",
    "    'learning_rate_initial': 0.1,\n",
    "    'learning_rate_decay_steps': int(10000000),\n",
    "    'learning_rate_decay': 1.0,\n",
    "    'start_step_early_stopping': 150000,\n",
    "    'early_stopping_patience': 1000.0,\n",
    "    'batch_size': 1000,\n",
    "    'max_steps': 100000000,\n",
    "    'average_n_validation_logloss': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create logloss_log to pickle and directories for TensorBoard and checkpoint\n",
    "logloss_log = prep_logloss_log(naming, variables)\n",
    "logloss_log['variables']['train_data_size'] = len(train_X)\n",
    "logloss_log['variables']['steps_per_epoch'] = int(len(train_X) / logloss_log['variables']['batch_size'])\n",
    "logloss_log['variables']['log_every'] = int(logloss_log['variables']['steps_per_epoch'] / 2)\n",
    "logloss_log['variables']['print_every'] = logloss_log['variables']['log_every'] * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_size = logloss_log['variables']['feature_size']\n",
    "weights_stddev = logloss_log['variables']['weights_stddev']\n",
    "biases_initial = logloss_log['variables']['biases_initial']\n",
    "first_hidden_layer_features = logloss_log['variables']['first_hidden_layer_features']\n",
    "second_hidden_layer_features = logloss_log['variables']['second_hidden_layer_features']\n",
    "third_hidden_layer_features = logloss_log['variables']['third_hidden_layer_features']\n",
    "learning_rate_initial = logloss_log['variables']['learning_rate_initial']\n",
    "learning_rate_decay_steps = logloss_log['variables']['learning_rate_decay_steps']\n",
    "learning_rate_decay = logloss_log['variables']['learning_rate_decay']\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    data = tf.placeholder(tf.float32, [None, feature_size])\n",
    "    labels = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    bn_1 = batch_normalize(data, is_training=is_training, global_step=global_step, scope='bn_data')\n",
    "    bn_1_drop = tf.nn.dropout(bn_1, keep_prob)\n",
    "    W_1 = weight_variable([feature_size, first_hidden_layer_features], weights_stddev, 'W_1')\n",
    "    b_1 = bias_variable(biases_initial, [first_hidden_layer_features], 'b_1')\n",
    "    output_1 = tf.matmul(bn_1_drop, W_1) + b_1\n",
    "    output_1_relu = tf.nn.relu(output_1)\n",
    "\n",
    "    bn_2 = batch_normalize(output_1_relu, is_training=is_training, global_step=global_step, scope='bn_output_1_relu')\n",
    "    bn_2_drop = tf.nn.dropout(bn_2, keep_prob)\n",
    "    W_2 = weight_variable([first_hidden_layer_features, second_hidden_layer_features], weights_stddev, 'W_2')\n",
    "    b_2 = bias_variable(biases_initial, [second_hidden_layer_features], 'b_2')\n",
    "    output_2 = tf.matmul(bn_2_drop, W_2) + b_2\n",
    "    output_2_relu = tf.nn.relu(output_2)\n",
    "\n",
    "    bn_3 = batch_normalize(output_2_relu, is_training=is_training, global_step=global_step, scope='bn_output_2_relu')\n",
    "    bn_3_drop = tf.nn.dropout(bn_3, keep_prob)\n",
    "    W_3 = weight_variable([second_hidden_layer_features, third_hidden_layer_features], weights_stddev, 'W_3')\n",
    "    b_3 = bias_variable(biases_initial, [third_hidden_layer_features], 'b_3')\n",
    "    output_3 = tf.matmul(bn_3_drop, W_3) + b_3\n",
    "    output_3_relu = tf.nn.relu(output_3)\n",
    "\n",
    "    bn_4 = batch_normalize(output_3_relu, is_training=is_training, global_step=global_step, scope='bn_output_3_relu')\n",
    "    bn_4_drop = tf.nn.dropout(bn_4, keep_prob)\n",
    "    W_4 = weight_variable([third_hidden_layer_features, 1], weights_stddev, 'W_4')\n",
    "    b_4 = bias_variable(biases_initial, [1], 'b_4')\n",
    "    logits = tf.matmul(bn_4_drop, W_4) + b_4\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate_initial, global_step, learning_rate_decay_steps, \n",
    "                                               learning_rate_decay, staircase=True)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    summarizer = tf.summary.merge_all()\n",
    "    \n",
    "graph_variables = {'graph': graph, \n",
    "                   'optimizer': optimizer, \n",
    "                   'summarizer': summarizer, \n",
    "                   'data': data, \n",
    "                   'labels': labels, \n",
    "                   'keep_prob': keep_prob, \n",
    "                   'is_training': is_training,\n",
    "                   'logloss': loss, \n",
    "                   'logits': logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING   START @ 2017-03-27 22:52:24.983272\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-0 >> ave validation logloss 0.70598\n",
      "STEP       0 END @ 2017-03-27 22:52:32.264155, training logloss 0.70555, ave validation logloss 0.70598\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-151 >> ave validation logloss 0.67889\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-302 >> ave validation logloss 0.66895\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-453 >> ave validation logloss 0.66319\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-604 >> ave validation logloss 0.65831\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-755 >> ave validation logloss 0.65378\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-906 >> ave validation logloss 0.64966\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1057 >> ave validation logloss 0.64580\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1208 >> ave validation logloss 0.63332\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1359 >> ave validation logloss 0.62718\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1510 >> ave validation logloss 0.62065\n",
      "STEP    1510 END @ 2017-03-27 22:54:51.240552, training logloss 0.59698, ave validation logloss 0.62065\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1661 >> ave validation logloss 0.61311\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1812 >> ave validation logloss 0.60648\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-1963 >> ave validation logloss 0.59983\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2114 >> ave validation logloss 0.59306\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2265 >> ave validation logloss 0.58674\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2416 >> ave validation logloss 0.58061\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2567 >> ave validation logloss 0.57516\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2718 >> ave validation logloss 0.56997\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-2869 >> ave validation logloss 0.56614\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3020 >> ave validation logloss 0.56152\n",
      "STEP    3020 END @ 2017-03-27 22:57:12.039943, training logloss 0.55612, ave validation logloss 0.56152\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3171 >> ave validation logloss 0.55679\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3322 >> ave validation logloss 0.55243\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3473 >> ave validation logloss 0.54837\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3624 >> ave validation logloss 0.54443\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3775 >> ave validation logloss 0.54028\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-3926 >> ave validation logloss 0.53679\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4077 >> ave validation logloss 0.53193\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4228 >> ave validation logloss 0.52700\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4379 >> ave validation logloss 0.52354\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4530 >> ave validation logloss 0.51954\n",
      "STEP    4530 END @ 2017-03-27 22:59:26.147484, training logloss 0.48982, ave validation logloss 0.51954\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4681 >> ave validation logloss 0.51477\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4832 >> ave validation logloss 0.51087\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-4983 >> ave validation logloss 0.50627\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5134 >> ave validation logloss 0.50071\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5285 >> ave validation logloss 0.49631\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5436 >> ave validation logloss 0.49206\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5587 >> ave validation logloss 0.48694\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5738 >> ave validation logloss 0.48267\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-5889 >> ave validation logloss 0.47887\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6040 >> ave validation logloss 0.47526\n",
      "STEP    6040 END @ 2017-03-27 23:01:48.396539, training logloss 0.45236, ave validation logloss 0.47526\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6191 >> ave validation logloss 0.47185\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6342 >> ave validation logloss 0.46848\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6493 >> ave validation logloss 0.46535\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6644 >> ave validation logloss 0.46246\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6795 >> ave validation logloss 0.45989\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-6946 >> ave validation logloss 0.45753\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7097 >> ave validation logloss 0.45535\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7248 >> ave validation logloss 0.45348\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7399 >> ave validation logloss 0.45170\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7550 >> ave validation logloss 0.45064\n",
      "STEP    7550 END @ 2017-03-27 23:04:10.703312, training logloss 0.46945, ave validation logloss 0.45064\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7701 >> ave validation logloss 0.44951\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-7852 >> ave validation logloss 0.44848\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8003 >> ave validation logloss 0.44766\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8154 >> ave validation logloss 0.44717\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8305 >> ave validation logloss 0.44669\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8456 >> ave validation logloss 0.44605\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8607 >> ave validation logloss 0.44535\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8758 >> ave validation logloss 0.44478\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-8909 >> ave validation logloss 0.44441\n",
      "STEP    9060 END @ 2017-03-27 23:06:28.923251, training logloss 0.42417, ave validation logloss 0.44459\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-9362 >> ave validation logloss 0.44412\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-9513 >> ave validation logloss 0.44373\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-9664 >> ave validation logloss 0.44329\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-10117 >> ave validation logloss 0.44324\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-10268 >> ave validation logloss 0.44269\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-10570 >> ave validation logloss 0.44252\n",
      "STEP   10570 END @ 2017-03-27 23:08:39.911246, training logloss 0.45618, ave validation logloss 0.44252\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-10721 >> ave validation logloss 0.44245\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-10872 >> ave validation logloss 0.44239\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-11023 >> ave validation logloss 0.44197\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-11627 >> ave validation logloss 0.44170\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-11778 >> ave validation logloss 0.44150\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-11929 >> ave validation logloss 0.44125\n",
      "STEP   12080 END @ 2017-03-27 23:10:43.466861, training logloss 0.45029, ave validation logloss 0.44134\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-12533 >> ave validation logloss 0.44098\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-12684 >> ave validation logloss 0.44083\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-12835 >> ave validation logloss 0.44067\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-13439 >> ave validation logloss 0.44051\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-13590 >> ave validation logloss 0.44033\n",
      "STEP   13590 END @ 2017-03-27 23:12:53.528133, training logloss 0.44273, ave validation logloss 0.44033\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-13741 >> ave validation logloss 0.43993\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-13892 >> ave validation logloss 0.43942\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-14043 >> ave validation logloss 0.43940\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-14194 >> ave validation logloss 0.43885\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-14345 >> ave validation logloss 0.43853\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-14496 >> ave validation logloss 0.43843\n",
      "STEP   15100 END @ 2017-03-27 23:15:00.611261, training logloss 0.46166, ave validation logloss 0.43982\n",
      "STEP   16610 END @ 2017-03-27 23:16:53.588555, training logloss 0.44503, ave validation logloss 0.43919\n",
      "STEP   18120 END @ 2017-03-27 23:18:45.708874, training logloss 0.43133, ave validation logloss 0.43858\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-18271 >> ave validation logloss 0.43841\n",
      "STEP   19630 END @ 2017-03-27 23:20:38.930948, training logloss 0.43050, ave validation logloss 0.43905\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-20385 >> ave validation logloss 0.43819\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-20687 >> ave validation logloss 0.43799\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-20989 >> ave validation logloss 0.43780\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-21140 >> ave validation logloss 0.43753\n",
      "STEP   21140 END @ 2017-03-27 23:22:47.265046, training logloss 0.43090, ave validation logloss 0.43753\n",
      "STEP   22650 END @ 2017-03-27 23:24:37.832240, training logloss 0.44833, ave validation logloss 0.43908\n",
      "STEP   24160 END @ 2017-03-27 23:26:28.330018, training logloss 0.41291, ave validation logloss 0.43830\n",
      "STEP   25670 END @ 2017-03-27 23:28:18.984174, training logloss 0.41093, ave validation logloss 0.43824\n",
      "STEP   27180 END @ 2017-03-27 23:30:09.578368, training logloss 0.44620, ave validation logloss 0.43787\n",
      "STEP   28690 END @ 2017-03-27 23:32:00.518134, training logloss 0.40256, ave validation logloss 0.43884\n",
      "STEP   30200 END @ 2017-03-27 23:33:51.436144, training logloss 0.41631, ave validation logloss 0.43781\n",
      "STEP   31710 END @ 2017-03-27 23:35:41.974179, training logloss 0.43260, ave validation logloss 0.43863\n",
      "STEP   33220 END @ 2017-03-27 23:37:32.923588, training logloss 0.42660, ave validation logloss 0.43773\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-33371 >> ave validation logloss 0.43746\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-33824 >> ave validation logloss 0.43742\n",
      "STEP   34730 END @ 2017-03-27 23:39:30.018725, training logloss 0.42416, ave validation logloss 0.43833\n",
      "STEP   36240 END @ 2017-03-27 23:41:21.173497, training logloss 0.41721, ave validation logloss 0.43883\n",
      "STEP   37750 END @ 2017-03-27 23:43:12.889522, training logloss 0.44032, ave validation logloss 0.43899\n",
      "STEP   39260 END @ 2017-03-27 23:45:03.436061, training logloss 0.44871, ave validation logloss 0.43867\n",
      "STEP   40770 END @ 2017-03-27 23:46:55.120471, training logloss 0.45678, ave validation logloss 0.43834\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-41374 >> ave validation logloss 0.43737\n",
      "STEP   42280 END @ 2017-03-27 23:48:48.486665, training logloss 0.42525, ave validation logloss 0.43802\n",
      "STEP   43790 END @ 2017-03-27 23:50:39.786665, training logloss 0.43984, ave validation logloss 0.43873\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-45300 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-45300 and pickle_nn_01 >>\n",
      "STEP   45300 END @ 2017-03-27 23:52:32.207488, training logloss 0.43507, ave validation logloss 0.43751\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-45451 >> ave validation logloss 0.43722\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-45904 >> ave validation logloss 0.43711\n",
      "STEP   46810 END @ 2017-03-27 23:54:27.048651, training logloss 0.41493, ave validation logloss 0.43796\n",
      "STEP   48320 END @ 2017-03-27 23:56:20.151284, training logloss 0.41716, ave validation logloss 0.43819\n",
      "STEP   49830 END @ 2017-03-27 23:58:12.501301, training logloss 0.42967, ave validation logloss 0.43811\n",
      "STEP   51340 END @ 2017-03-28 00:00:03.289317, training logloss 0.44439, ave validation logloss 0.43830\n",
      "STEP   52850 END @ 2017-03-28 00:01:54.424652, training logloss 0.43773, ave validation logloss 0.43895\n",
      "STEP   54360 END @ 2017-03-28 00:03:46.960465, training logloss 0.41459, ave validation logloss 0.43869\n",
      "STEP   55870 END @ 2017-03-28 00:05:38.938208, training logloss 0.41093, ave validation logloss 0.43813\n",
      "STEP   57380 END @ 2017-03-28 00:07:30.084164, training logloss 0.43064, ave validation logloss 0.43747\n",
      "STEP   58890 END @ 2017-03-28 00:09:21.019620, training logloss 0.42016, ave validation logloss 0.43835\n",
      "STEP   60400 END @ 2017-03-28 00:11:10.938125, training logloss 0.43348, ave validation logloss 0.43909\n",
      "STEP   61910 END @ 2017-03-28 00:13:01.257324, training logloss 0.43422, ave validation logloss 0.43824\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-63269 >> ave validation logloss 0.43688\n",
      "STEP   63420 END @ 2017-03-28 00:14:53.318065, training logloss 0.42545, ave validation logloss 0.43727\n",
      "STEP   64930 END @ 2017-03-28 00:16:42.891082, training logloss 0.41740, ave validation logloss 0.43764\n",
      "STEP   66440 END @ 2017-03-28 00:18:33.287535, training logloss 0.44970, ave validation logloss 0.43744\n",
      "STEP   67950 END @ 2017-03-28 00:20:23.810678, training logloss 0.43047, ave validation logloss 0.43780\n",
      "STEP   69460 END @ 2017-03-28 00:22:13.726525, training logloss 0.42137, ave validation logloss 0.43728\n",
      "STEP   70970 END @ 2017-03-28 00:24:03.878719, training logloss 0.41646, ave validation logloss 0.43759\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-71272 >> ave validation logloss 0.43680\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-71423 >> ave validation logloss 0.43620\n",
      "STEP   72480 END @ 2017-03-28 00:26:06.082150, training logloss 0.42415, ave validation logloss 0.43776\n",
      "STEP   73990 END @ 2017-03-28 00:27:56.739173, training logloss 0.44661, ave validation logloss 0.43748\n",
      "STEP   75500 END @ 2017-03-28 00:29:46.910205, training logloss 0.43700, ave validation logloss 0.43758\n",
      "STEP   77010 END @ 2017-03-28 00:31:37.944993, training logloss 0.41943, ave validation logloss 0.43749\n",
      "STEP   78520 END @ 2017-03-28 00:33:27.835568, training logloss 0.42831, ave validation logloss 0.43741\n",
      "STEP   80030 END @ 2017-03-28 00:35:18.078703, training logloss 0.39243, ave validation logloss 0.43791\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-81238 >> ave validation logloss 0.43607\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-81540 >> ave validation logloss 0.43588\n",
      "STEP   81540 END @ 2017-03-28 00:37:12.328637, training logloss 0.43490, ave validation logloss 0.43588\n",
      "STEP   83050 END @ 2017-03-28 00:39:02.198205, training logloss 0.39883, ave validation logloss 0.43740\n",
      "STEP   84560 END @ 2017-03-28 00:40:52.480010, training logloss 0.40715, ave validation logloss 0.43648\n",
      "STEP   86070 END @ 2017-03-28 00:42:42.889371, training logloss 0.44498, ave validation logloss 0.43722\n",
      "STEP   87580 END @ 2017-03-28 00:44:33.115659, training logloss 0.42810, ave validation logloss 0.43710\n",
      "STEP   89090 END @ 2017-03-28 00:46:23.545594, training logloss 0.41973, ave validation logloss 0.43677\n",
      "STEP   90600 END @ 2017-03-28 00:48:14.645994, training logloss 0.43062, ave validation logloss 0.43649\n",
      "STEP   92110 END @ 2017-03-28 00:50:04.771006, training logloss 0.43273, ave validation logloss 0.43700\n",
      "STEP   93620 END @ 2017-03-28 00:51:55.090073, training logloss 0.42904, ave validation logloss 0.43618\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-93771 >> ave validation logloss 0.43576\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-93922 >> ave validation logloss 0.43571\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-94073 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-94073 and pickle_nn_01 >>\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-94073 >> ave validation logloss 0.43554\n",
      "STEP   95130 END @ 2017-03-28 00:53:56.236668, training logloss 0.42411, ave validation logloss 0.43574\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-96338 >> ave validation logloss 0.43548\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-96489 >> ave validation logloss 0.43533\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-96640 >> ave validation logloss 0.43510\n",
      "STEP   96640 END @ 2017-03-28 00:55:53.137971, training logloss 0.41972, ave validation logloss 0.43510\n",
      "STEP   98150 END @ 2017-03-28 00:57:43.579438, training logloss 0.42256, ave validation logloss 0.43589\n",
      "STEP   99660 END @ 2017-03-28 00:59:33.857063, training logloss 0.42274, ave validation logloss 0.43581\n",
      "STEP  101170 END @ 2017-03-28 01:01:24.871411, training logloss 0.44309, ave validation logloss 0.43600\n",
      "STEP  102680 END @ 2017-03-28 01:03:15.263908, training logloss 0.42810, ave validation logloss 0.43640\n",
      "STEP  104190 END @ 2017-03-28 01:05:05.744867, training logloss 0.40763, ave validation logloss 0.43631\n",
      "STEP  105700 END @ 2017-03-28 01:06:56.395420, training logloss 0.41685, ave validation logloss 0.43601\n",
      "STEP  107210 END @ 2017-03-28 01:08:46.511938, training logloss 0.43978, ave validation logloss 0.43617\n",
      "STEP  108720 END @ 2017-03-28 01:10:37.139615, training logloss 0.41606, ave validation logloss 0.43636\n",
      "STEP  110230 END @ 2017-03-28 01:12:27.234197, training logloss 0.41301, ave validation logloss 0.43648\n",
      "STEP  111740 END @ 2017-03-28 01:14:17.183381, training logloss 0.42884, ave validation logloss 0.43617\n",
      "STEP  113250 END @ 2017-03-28 01:16:07.265377, training logloss 0.42430, ave validation logloss 0.43639\n",
      "STEP  114760 END @ 2017-03-28 01:17:57.206044, training logloss 0.41033, ave validation logloss 0.43553\n",
      "STEP  116270 END @ 2017-03-28 01:19:47.928870, training logloss 0.41020, ave validation logloss 0.43601\n",
      "STEP  117780 END @ 2017-03-28 01:21:38.717977, training logloss 0.44209, ave validation logloss 0.43623\n",
      "STEP  119290 END @ 2017-03-28 01:23:29.434027, training logloss 0.41323, ave validation logloss 0.43531\n",
      "STEP  120800 END @ 2017-03-28 01:25:20.129208, training logloss 0.42062, ave validation logloss 0.43553\n",
      "STEP  122310 END @ 2017-03-28 01:27:10.799162, training logloss 0.41551, ave validation logloss 0.43590\n",
      "STEP  123820 END @ 2017-03-28 01:29:01.548937, training logloss 0.42725, ave validation logloss 0.43635\n",
      "STEP  125330 END @ 2017-03-28 01:30:52.201124, training logloss 0.38981, ave validation logloss 0.43589\n",
      "STEP  126840 END @ 2017-03-28 01:32:42.625322, training logloss 0.40008, ave validation logloss 0.43576\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-127746 >> ave validation logloss 0.43488\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-127897 >> ave validation logloss 0.43451\n",
      "STEP  128350 END @ 2017-03-28 01:34:44.720270, training logloss 0.42186, ave validation logloss 0.43454\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-128501 >> ave validation logloss 0.43420\n",
      "STEP  129860 END @ 2017-03-28 01:36:37.370827, training logloss 0.43318, ave validation logloss 0.43516\n",
      "STEP  131370 END @ 2017-03-28 01:38:27.696216, training logloss 0.42022, ave validation logloss 0.43585\n",
      "STEP  132880 END @ 2017-03-28 01:40:18.024192, training logloss 0.41166, ave validation logloss 0.43519\n",
      "STEP  134390 END @ 2017-03-28 01:42:08.411324, training logloss 0.43176, ave validation logloss 0.43535\n",
      "STEP  135900 END @ 2017-03-28 01:43:58.966203, training logloss 0.40124, ave validation logloss 0.43513\n",
      "STEP  137410 END @ 2017-03-28 01:45:49.026398, training logloss 0.42596, ave validation logloss 0.43589\n",
      "STEP  138920 END @ 2017-03-28 01:47:38.900537, training logloss 0.41293, ave validation logloss 0.43612\n",
      "STEP  140430 END @ 2017-03-28 01:49:29.828382, training logloss 0.43260, ave validation logloss 0.43422\n",
      "STEP  141940 END @ 2017-03-28 01:51:20.750400, training logloss 0.42428, ave validation logloss 0.43553\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-142997 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-142997 and pickle_nn_01 >>\n",
      "STEP  143450 END @ 2017-03-28 01:53:13.615581, training logloss 0.43945, ave validation logloss 0.43487\n",
      "STEP  144960 END @ 2017-03-28 01:55:04.389223, training logloss 0.42970, ave validation logloss 0.43461\n",
      "STEP  146470 END @ 2017-03-28 01:56:54.624344, training logloss 0.42222, ave validation logloss 0.43499\n",
      "STEP  147980 END @ 2017-03-28 01:58:45.755470, training logloss 0.42031, ave validation logloss 0.43514\n",
      "STEP  149490 END @ 2017-03-28 02:00:36.541954, training logloss 0.44523, ave validation logloss 0.43472\n",
      "STEP  151000 END @ 2017-03-28 02:02:27.233773, training logloss 0.40355, ave validation logloss 0.43471\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-151453 >> ave validation logloss 0.43412\n",
      "STEP  152510 END @ 2017-03-28 02:04:19.698793, training logloss 0.44158, ave validation logloss 0.43510\n",
      "STEP  154020 END @ 2017-03-28 02:06:10.062915, training logloss 0.42787, ave validation logloss 0.43580\n",
      "STEP  155530 END @ 2017-03-28 02:08:00.454435, training logloss 0.42626, ave validation logloss 0.43439\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-156738 >> ave validation logloss 0.43411\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-156889 >> ave validation logloss 0.43390\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-157040 >> ave validation logloss 0.43378\n",
      "STEP  157040 END @ 2017-03-28 02:09:57.160899, training logloss 0.42089, ave validation logloss 0.43378\n",
      "STEP  158550 END @ 2017-03-28 02:11:47.830636, training logloss 0.46232, ave validation logloss 0.43504\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-159909 >> ave validation logloss 0.43342\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-160060 >> ave validation logloss 0.43332\n",
      "STEP  160060 END @ 2017-03-28 02:13:42.214255, training logloss 0.41163, ave validation logloss 0.43332\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-160211 >> ave validation logloss 0.43303\n",
      "STEP  161570 END @ 2017-03-28 02:15:34.777080, training logloss 0.41768, ave validation logloss 0.43478\n",
      "STEP  163080 END @ 2017-03-28 02:17:26.099807, training logloss 0.44820, ave validation logloss 0.43419\n",
      "STEP  164590 END @ 2017-03-28 02:19:17.164235, training logloss 0.40947, ave validation logloss 0.43479\n",
      "STEP  166100 END @ 2017-03-28 02:21:07.782895, training logloss 0.42385, ave validation logloss 0.43350\n",
      "STEP  167610 END @ 2017-03-28 02:22:58.768122, training logloss 0.41659, ave validation logloss 0.43443\n",
      "STEP  169120 END @ 2017-03-28 02:24:49.836494, training logloss 0.41605, ave validation logloss 0.43403\n",
      "STEP  170630 END @ 2017-03-28 02:26:40.411626, training logloss 0.41608, ave validation logloss 0.43516\n",
      "STEP  172140 END @ 2017-03-28 02:28:30.846473, training logloss 0.40140, ave validation logloss 0.43364\n",
      "STEP  173650 END @ 2017-03-28 02:30:21.551225, training logloss 0.39347, ave validation logloss 0.43403\n",
      "STEP  175160 END @ 2017-03-28 02:32:12.064454, training logloss 0.43046, ave validation logloss 0.43414\n",
      "STEP  176670 END @ 2017-03-28 02:34:02.297333, training logloss 0.43118, ave validation logloss 0.43505\n",
      "STEP  178180 END @ 2017-03-28 02:35:52.870064, training logloss 0.41000, ave validation logloss 0.43386\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-178633 >> ave validation logloss 0.43278\n",
      "STEP  179690 END @ 2017-03-28 02:37:52.154844, training logloss 0.41441, ave validation logloss 0.43456\n",
      "STEP  181200 END @ 2017-03-28 02:39:42.351148, training logloss 0.42443, ave validation logloss 0.43516\n",
      "STEP  182710 END @ 2017-03-28 02:41:32.563602, training logloss 0.44635, ave validation logloss 0.43399\n",
      "STEP  184220 END @ 2017-03-28 02:43:22.792623, training logloss 0.42518, ave validation logloss 0.43398\n",
      "STEP  185730 END @ 2017-03-28 02:45:14.100332, training logloss 0.42552, ave validation logloss 0.43436\n",
      "STEP  187240 END @ 2017-03-28 02:47:05.117033, training logloss 0.41897, ave validation logloss 0.43426\n",
      "STEP  188750 END @ 2017-03-28 02:48:55.998653, training logloss 0.40657, ave validation logloss 0.43340\n",
      "STEP  190260 END @ 2017-03-28 02:50:46.999801, training logloss 0.43967, ave validation logloss 0.43406\n",
      "STEP  191770 END @ 2017-03-28 02:52:37.934890, training logloss 0.44265, ave validation logloss 0.43545\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-191921 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-191921 and pickle_nn_01 >>\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-193280 >> ave validation logloss 0.43264\n",
      "STEP  193280 END @ 2017-03-28 02:54:32.891422, training logloss 0.40784, ave validation logloss 0.43264\n",
      "STEP  194790 END @ 2017-03-28 02:56:23.599679, training logloss 0.41804, ave validation logloss 0.43282\n",
      "STEP  196300 END @ 2017-03-28 02:58:14.255235, training logloss 0.40521, ave validation logloss 0.43410\n",
      "STEP  197810 END @ 2017-03-28 03:00:04.598194, training logloss 0.41812, ave validation logloss 0.43337\n",
      "STEP  199320 END @ 2017-03-28 03:01:55.061584, training logloss 0.42332, ave validation logloss 0.43315\n",
      "STEP  200830 END @ 2017-03-28 03:03:45.645688, training logloss 0.38814, ave validation logloss 0.43301\n",
      "STEP  202340 END @ 2017-03-28 03:05:36.251783, training logloss 0.40357, ave validation logloss 0.43493\n",
      "STEP  203850 END @ 2017-03-28 03:07:26.523888, training logloss 0.41835, ave validation logloss 0.43353\n",
      "STEP  205360 END @ 2017-03-28 03:09:16.808642, training logloss 0.42448, ave validation logloss 0.43315\n",
      "STEP  206870 END @ 2017-03-28 03:11:07.093716, training logloss 0.41864, ave validation logloss 0.43321\n",
      "STEP  208380 END @ 2017-03-28 03:12:58.321089, training logloss 0.42054, ave validation logloss 0.43312\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-209739 >> ave validation logloss 0.43262\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-209890 >> ave validation logloss 0.43261\n",
      "STEP  209890 END @ 2017-03-28 03:14:53.308197, training logloss 0.40807, ave validation logloss 0.43261\n",
      "STEP  211400 END @ 2017-03-28 03:16:44.108797, training logloss 0.39609, ave validation logloss 0.43273\n",
      "STEP  212910 END @ 2017-03-28 03:18:35.308997, training logloss 0.42735, ave validation logloss 0.43304\n",
      "STEP  214420 END @ 2017-03-28 03:20:25.944774, training logloss 0.38606, ave validation logloss 0.43458\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-215326 >> ave validation logloss 0.43242\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-215477 >> ave validation logloss 0.43223\n",
      "STEP  215930 END @ 2017-03-28 03:22:20.805899, training logloss 0.40861, ave validation logloss 0.43315\n",
      "STEP  217440 END @ 2017-03-28 03:24:11.325885, training logloss 0.41014, ave validation logloss 0.43237\n",
      "STEP  218950 END @ 2017-03-28 03:26:01.629502, training logloss 0.38449, ave validation logloss 0.43275\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-219252 >> ave validation logloss 0.43221\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-219554 >> ave validation logloss 0.43188\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-219705 >> ave validation logloss 0.43158\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-220007 >> ave validation logloss 0.43116\n",
      "STEP  220460 END @ 2017-03-28 03:28:08.064851, training logloss 0.40059, ave validation logloss 0.43149\n",
      "STEP  221970 END @ 2017-03-28 03:29:58.630507, training logloss 0.42229, ave validation logloss 0.43236\n",
      "STEP  223480 END @ 2017-03-28 03:31:49.193348, training logloss 0.42873, ave validation logloss 0.43265\n",
      "STEP  224990 END @ 2017-03-28 03:33:39.398629, training logloss 0.42401, ave validation logloss 0.43296\n",
      "STEP  226500 END @ 2017-03-28 03:35:29.948858, training logloss 0.42371, ave validation logloss 0.43278\n",
      "STEP  228010 END @ 2017-03-28 03:37:19.935524, training logloss 0.40773, ave validation logloss 0.43353\n",
      "STEP  229520 END @ 2017-03-28 03:39:09.983324, training logloss 0.41521, ave validation logloss 0.43292\n",
      "STEP  231030 END @ 2017-03-28 03:41:00.355815, training logloss 0.41394, ave validation logloss 0.43248\n",
      "STEP  232540 END @ 2017-03-28 03:42:51.330270, training logloss 0.42915, ave validation logloss 0.43193\n",
      "STEP  234050 END @ 2017-03-28 03:44:42.210978, training logloss 0.41321, ave validation logloss 0.43275\n",
      "STEP  235560 END @ 2017-03-28 03:46:32.822547, training logloss 0.41265, ave validation logloss 0.43271\n",
      "STEP  237070 END @ 2017-03-28 03:48:24.207408, training logloss 0.42219, ave validation logloss 0.43283\n",
      "STEP  238580 END @ 2017-03-28 03:50:14.562574, training logloss 0.43865, ave validation logloss 0.43286\n",
      "STEP  240090 END @ 2017-03-28 03:52:05.025967, training logloss 0.43539, ave validation logloss 0.43231\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-240845 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-240845 and pickle_nn_01 >>\n",
      "STEP  241600 END @ 2017-03-28 03:53:57.809683, training logloss 0.41518, ave validation logloss 0.43231\n",
      "STEP  243110 END @ 2017-03-28 03:55:48.048511, training logloss 0.40559, ave validation logloss 0.43216\n",
      "STEP  244620 END @ 2017-03-28 03:57:38.153648, training logloss 0.41405, ave validation logloss 0.43418\n",
      "STEP  246130 END @ 2017-03-28 03:59:28.821284, training logloss 0.41069, ave validation logloss 0.43213\n",
      "STEP  247640 END @ 2017-03-28 04:01:18.810191, training logloss 0.41540, ave validation logloss 0.43124\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-248244 >> ave validation logloss 0.43095\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-248395 >> ave validation logloss 0.43081\n",
      "STEP  249150 END @ 2017-03-28 04:03:13.114263, training logloss 0.41103, ave validation logloss 0.43170\n",
      "STEP  250660 END @ 2017-03-28 04:05:03.110848, training logloss 0.40593, ave validation logloss 0.43156\n",
      "STEP  252170 END @ 2017-03-28 04:06:53.313332, training logloss 0.39549, ave validation logloss 0.43120\n",
      "STEP  253680 END @ 2017-03-28 04:08:44.346584, training logloss 0.40128, ave validation logloss 0.43162\n",
      "STEP  255190 END @ 2017-03-28 04:10:35.808166, training logloss 0.42538, ave validation logloss 0.43295\n",
      "STEP  256700 END @ 2017-03-28 04:12:26.960568, training logloss 0.40607, ave validation logloss 0.43258\n",
      "STEP  258210 END @ 2017-03-28 04:14:17.532385, training logloss 0.42720, ave validation logloss 0.43297\n",
      "STEP  259720 END @ 2017-03-28 04:16:07.976166, training logloss 0.40543, ave validation logloss 0.43224\n",
      "STEP  261230 END @ 2017-03-28 04:17:58.873322, training logloss 0.43084, ave validation logloss 0.43167\n",
      "STEP  262740 END @ 2017-03-28 04:19:49.442281, training logloss 0.42983, ave validation logloss 0.43280\n",
      "STEP  264250 END @ 2017-03-28 04:21:39.776067, training logloss 0.42782, ave validation logloss 0.43194\n",
      "STEP  265760 END @ 2017-03-28 04:23:30.393463, training logloss 0.41633, ave validation logloss 0.43181\n",
      "STEP  267270 END @ 2017-03-28 04:25:20.550890, training logloss 0.42026, ave validation logloss 0.43155\n",
      "STEP  268780 END @ 2017-03-28 04:27:10.690645, training logloss 0.40951, ave validation logloss 0.43288\n",
      "STEP  270290 END @ 2017-03-28 04:29:01.053236, training logloss 0.39960, ave validation logloss 0.43199\n",
      "STEP  271800 END @ 2017-03-28 04:30:51.307482, training logloss 0.39500, ave validation logloss 0.43143\n",
      "STEP  273310 END @ 2017-03-28 04:32:41.338368, training logloss 0.41268, ave validation logloss 0.43152\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-274669 >> ave validation logloss 0.43069\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-274820 >> ave validation logloss 0.43066\n",
      "STEP  274820 END @ 2017-03-28 04:34:36.069570, training logloss 0.40514, ave validation logloss 0.43066\n",
      "STEP  276330 END @ 2017-03-28 04:36:25.887629, training logloss 0.42985, ave validation logloss 0.43191\n",
      "STEP  277840 END @ 2017-03-28 04:38:16.899018, training logloss 0.39703, ave validation logloss 0.43156\n",
      "STEP  279350 END @ 2017-03-28 04:40:07.669231, training logloss 0.41557, ave validation logloss 0.43279\n",
      "STEP  280860 END @ 2017-03-28 04:41:58.428481, training logloss 0.40052, ave validation logloss 0.43204\n",
      "STEP  282370 END @ 2017-03-28 04:43:49.136082, training logloss 0.41230, ave validation logloss 0.43167\n",
      "STEP  283880 END @ 2017-03-28 04:45:39.918747, training logloss 0.42118, ave validation logloss 0.43218\n",
      "STEP  285390 END @ 2017-03-28 04:47:30.701399, training logloss 0.38962, ave validation logloss 0.43233\n",
      "STEP  286900 END @ 2017-03-28 04:49:21.704605, training logloss 0.37357, ave validation logloss 0.43090\n",
      "STEP  288410 END @ 2017-03-28 04:51:12.076689, training logloss 0.39989, ave validation logloss 0.43154\n",
      "STEP  289920 END @ 2017-03-28 04:53:02.581048, training logloss 0.41711, ave validation logloss 0.43154\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-290071 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-290071 and pickle_nn_01 >>\n",
      "STEP  291430 END @ 2017-03-28 04:54:55.118170, training logloss 0.38839, ave validation logloss 0.43097\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-291732 >> ave validation logloss 0.43051\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-292034 >> ave validation logloss 0.43037\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-292185 >> ave validation logloss 0.43003\n",
      "STEP  292940 END @ 2017-03-28 04:56:51.936847, training logloss 0.41924, ave validation logloss 0.43091\n",
      "STEP  294450 END @ 2017-03-28 04:58:41.901729, training logloss 0.40579, ave validation logloss 0.43105\n",
      "STEP  295960 END @ 2017-03-28 05:00:32.396683, training logloss 0.42243, ave validation logloss 0.43170\n",
      "STEP  297470 END @ 2017-03-28 05:02:22.795379, training logloss 0.41169, ave validation logloss 0.43066\n",
      "STEP  298980 END @ 2017-03-28 05:04:13.265155, training logloss 0.38379, ave validation logloss 0.43011\n",
      "STEP  300490 END @ 2017-03-28 05:06:04.175700, training logloss 0.42141, ave validation logloss 0.43139\n",
      "STEP  302000 END @ 2017-03-28 05:07:55.334197, training logloss 0.43047, ave validation logloss 0.43144\n",
      "STEP  303510 END @ 2017-03-28 05:09:46.488822, training logloss 0.42720, ave validation logloss 0.43067\n",
      "STEP  305020 END @ 2017-03-28 05:11:37.062052, training logloss 0.43230, ave validation logloss 0.43151\n",
      "STEP  306530 END @ 2017-03-28 05:13:27.535885, training logloss 0.42053, ave validation logloss 0.43209\n",
      "STEP  308040 END @ 2017-03-28 05:15:18.198339, training logloss 0.42143, ave validation logloss 0.43130\n",
      "STEP  309550 END @ 2017-03-28 05:17:09.046929, training logloss 0.41349, ave validation logloss 0.43084\n",
      "STEP  311060 END @ 2017-03-28 05:18:59.699689, training logloss 0.42372, ave validation logloss 0.43251\n",
      "STEP  312570 END @ 2017-03-28 05:20:49.688562, training logloss 0.42622, ave validation logloss 0.43150\n",
      "STEP  314080 END @ 2017-03-28 05:22:40.425857, training logloss 0.41092, ave validation logloss 0.43032\n",
      "STEP  315590 END @ 2017-03-28 05:24:31.052754, training logloss 0.41128, ave validation logloss 0.43165\n",
      "STEP  317100 END @ 2017-03-28 05:26:21.733977, training logloss 0.41737, ave validation logloss 0.43189\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-318006 >> ave validation logloss 0.42983\n",
      "STEP  318610 END @ 2017-03-28 05:28:14.317101, training logloss 0.43502, ave validation logloss 0.43047\n",
      "STEP  320120 END @ 2017-03-28 05:30:04.543920, training logloss 0.41591, ave validation logloss 0.43116\n",
      "STEP  321630 END @ 2017-03-28 05:31:54.606172, training logloss 0.42096, ave validation logloss 0.43023\n",
      "STEP  323140 END @ 2017-03-28 05:33:45.618303, training logloss 0.42960, ave validation logloss 0.43192\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-324650 >> ave validation logloss 0.42958\n",
      "STEP  324650 END @ 2017-03-28 05:35:45.937934, training logloss 0.39837, ave validation logloss 0.42958\n",
      "STEP  326160 END @ 2017-03-28 05:37:36.809047, training logloss 0.41551, ave validation logloss 0.43067\n",
      "STEP  327670 END @ 2017-03-28 05:39:28.090782, training logloss 0.41107, ave validation logloss 0.43121\n",
      "STEP  329180 END @ 2017-03-28 05:41:18.649469, training logloss 0.41075, ave validation logloss 0.43054\n",
      "STEP  330690 END @ 2017-03-28 05:43:09.458537, training logloss 0.42088, ave validation logloss 0.43160\n",
      "STEP  332200 END @ 2017-03-28 05:45:00.390478, training logloss 0.41776, ave validation logloss 0.43141\n",
      "STEP  333710 END @ 2017-03-28 05:46:50.987518, training logloss 0.42754, ave validation logloss 0.43035\n",
      "STEP  335220 END @ 2017-03-28 05:48:41.237099, training logloss 0.41004, ave validation logloss 0.43161\n",
      "STEP  336730 END @ 2017-03-28 05:50:31.868056, training logloss 0.42539, ave validation logloss 0.43150\n",
      "STEP  338240 END @ 2017-03-28 05:52:22.266208, training logloss 0.39891, ave validation logloss 0.43103\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-338995 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-338995 and pickle_nn_01 >>\n",
      "STEP  339750 END @ 2017-03-28 05:54:14.587574, training logloss 0.40380, ave validation logloss 0.43233\n",
      "STEP  341260 END @ 2017-03-28 05:56:04.725095, training logloss 0.41161, ave validation logloss 0.43084\n",
      "STEP  342770 END @ 2017-03-28 05:57:54.753702, training logloss 0.40256, ave validation logloss 0.43050\n",
      "STEP  344280 END @ 2017-03-28 05:59:44.825415, training logloss 0.43330, ave validation logloss 0.43046\n",
      "STEP  345790 END @ 2017-03-28 06:01:35.561682, training logloss 0.41839, ave validation logloss 0.43102\n",
      "STEP  347300 END @ 2017-03-28 06:03:26.408044, training logloss 0.40935, ave validation logloss 0.42979\n",
      "STEP  348810 END @ 2017-03-28 06:05:17.449888, training logloss 0.43068, ave validation logloss 0.43080\n",
      "STEP  350320 END @ 2017-03-28 06:07:08.162848, training logloss 0.43280, ave validation logloss 0.43171\n",
      "STEP  351830 END @ 2017-03-28 06:08:58.886387, training logloss 0.38781, ave validation logloss 0.43180\n",
      "STEP  353340 END @ 2017-03-28 06:10:49.545467, training logloss 0.41651, ave validation logloss 0.43127\n",
      "STEP  354850 END @ 2017-03-28 06:12:39.901463, training logloss 0.41889, ave validation logloss 0.43066\n",
      "STEP  356360 END @ 2017-03-28 06:14:30.372935, training logloss 0.39787, ave validation logloss 0.42968\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-356511 >> ave validation logloss 0.42958\n",
      "STEP  357870 END @ 2017-03-28 06:16:23.528036, training logloss 0.41899, ave validation logloss 0.43011\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-358172 >> ave validation logloss 0.42939\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-358474 >> ave validation logloss 0.42931\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-358625 >> ave validation logloss 0.42929\n",
      "STEP  359380 END @ 2017-03-28 06:18:20.617471, training logloss 0.40137, ave validation logloss 0.43055\n",
      "STEP  360890 END @ 2017-03-28 06:20:10.883176, training logloss 0.44211, ave validation logloss 0.43078\n",
      "STEP  362400 END @ 2017-03-28 06:22:00.848684, training logloss 0.41501, ave validation logloss 0.43004\n",
      "STEP  363910 END @ 2017-03-28 06:23:50.805221, training logloss 0.41233, ave validation logloss 0.43036\n",
      "STEP  365420 END @ 2017-03-28 06:25:41.316432, training logloss 0.42007, ave validation logloss 0.43139\n",
      "STEP  366930 END @ 2017-03-28 06:27:31.360279, training logloss 0.41040, ave validation logloss 0.43039\n",
      "STEP  368440 END @ 2017-03-28 06:29:21.416414, training logloss 0.40971, ave validation logloss 0.43005\n",
      "STEP  369950 END @ 2017-03-28 06:31:12.861262, training logloss 0.42681, ave validation logloss 0.43041\n",
      "STEP  371460 END @ 2017-03-28 06:33:03.652255, training logloss 0.42111, ave validation logloss 0.42999\n",
      "STEP  372970 END @ 2017-03-28 06:34:54.688975, training logloss 0.41632, ave validation logloss 0.43075\n",
      "STEP  374480 END @ 2017-03-28 06:36:45.430596, training logloss 0.41031, ave validation logloss 0.43058\n",
      "STEP  375990 END @ 2017-03-28 06:38:36.244401, training logloss 0.43450, ave validation logloss 0.43061\n",
      "STEP  377500 END @ 2017-03-28 06:40:26.942512, training logloss 0.41064, ave validation logloss 0.43081\n",
      "STEP  379010 END @ 2017-03-28 06:42:17.891522, training logloss 0.41638, ave validation logloss 0.43032\n",
      "STEP  380520 END @ 2017-03-28 06:44:08.596036, training logloss 0.42617, ave validation logloss 0.43114\n",
      "STEP  382030 END @ 2017-03-28 06:45:59.112809, training logloss 0.39496, ave validation logloss 0.42987\n",
      "STEP  383540 END @ 2017-03-28 06:47:49.937821, training logloss 0.42444, ave validation logloss 0.43003\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-383993 >> ave validation logloss 0.42895\n",
      "STEP  385050 END @ 2017-03-28 06:49:42.539731, training logloss 0.42185, ave validation logloss 0.43046\n",
      "STEP  386560 END @ 2017-03-28 06:51:32.969742, training logloss 0.44459, ave validation logloss 0.43009\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-388070 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-388070 and pickle_nn_01 >>\n",
      "STEP  388070 END @ 2017-03-28 06:53:25.538858, training logloss 0.40685, ave validation logloss 0.43053\n",
      "STEP  389580 END @ 2017-03-28 06:55:15.618105, training logloss 0.39010, ave validation logloss 0.43107\n",
      "STEP  391090 END @ 2017-03-28 06:57:06.094297, training logloss 0.41431, ave validation logloss 0.42940\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-391241 >> ave validation logloss 0.42887\n",
      "STEP  392600 END @ 2017-03-28 06:58:59.460344, training logloss 0.42247, ave validation logloss 0.42917\n",
      "STEP  394110 END @ 2017-03-28 07:00:50.676128, training logloss 0.41332, ave validation logloss 0.43019\n",
      "STEP  395620 END @ 2017-03-28 07:02:41.321220, training logloss 0.42317, ave validation logloss 0.43006\n",
      "STEP  397130 END @ 2017-03-28 07:04:32.027722, training logloss 0.40005, ave validation logloss 0.42976\n",
      "STEP  398640 END @ 2017-03-28 07:06:22.754610, training logloss 0.40924, ave validation logloss 0.42999\n",
      "STEP  400150 END @ 2017-03-28 07:08:13.257893, training logloss 0.40843, ave validation logloss 0.43189\n",
      "STEP  401660 END @ 2017-03-28 07:10:03.668940, training logloss 0.41736, ave validation logloss 0.43044\n",
      "STEP  403170 END @ 2017-03-28 07:11:53.970519, training logloss 0.40072, ave validation logloss 0.43078\n",
      "STEP  404680 END @ 2017-03-28 07:13:44.673119, training logloss 0.40256, ave validation logloss 0.42964\n",
      "STEP  406190 END @ 2017-03-28 07:15:35.200746, training logloss 0.43089, ave validation logloss 0.43030\n",
      "STEP  407700 END @ 2017-03-28 07:17:25.458810, training logloss 0.41991, ave validation logloss 0.43057\n",
      "STEP  409210 END @ 2017-03-28 07:19:15.868529, training logloss 0.40446, ave validation logloss 0.43056\n",
      "STEP  410720 END @ 2017-03-28 07:21:06.663264, training logloss 0.40437, ave validation logloss 0.42980\n",
      "STEP  412230 END @ 2017-03-28 07:22:56.597388, training logloss 0.41288, ave validation logloss 0.43037\n",
      "STEP  413740 END @ 2017-03-28 07:24:45.964217, training logloss 0.40838, ave validation logloss 0.42969\n",
      "STEP  415250 END @ 2017-03-28 07:26:36.584684, training logloss 0.43106, ave validation logloss 0.42998\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-416156 >> ave validation logloss 0.42877\n",
      "STEP  416760 END @ 2017-03-28 07:28:27.385748, training logloss 0.39905, ave validation logloss 0.42927\n",
      "STEP  418270 END @ 2017-03-28 07:30:15.700487, training logloss 0.41096, ave validation logloss 0.42930\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-418421 >> ave validation logloss 0.42871\n",
      "STEP  419780 END @ 2017-03-28 07:32:09.962790, training logloss 0.40559, ave validation logloss 0.42984\n",
      "STEP  421290 END @ 2017-03-28 07:34:02.556507, training logloss 0.40124, ave validation logloss 0.42926\n",
      "STEP  422800 END @ 2017-03-28 07:35:52.886008, training logloss 0.38081, ave validation logloss 0.42928\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-423253 >> ave validation logloss 0.42865\n",
      "STEP  424310 END @ 2017-03-28 07:37:52.565711, training logloss 0.41665, ave validation logloss 0.42957\n",
      "STEP  425820 END @ 2017-03-28 07:39:43.022355, training logloss 0.39638, ave validation logloss 0.42944\n",
      "STEP  427330 END @ 2017-03-28 07:41:33.225113, training logloss 0.40729, ave validation logloss 0.42998\n",
      "STEP  428840 END @ 2017-03-28 07:43:23.802224, training logloss 0.40088, ave validation logloss 0.42982\n",
      "STEP  430350 END @ 2017-03-28 07:45:14.310359, training logloss 0.43657, ave validation logloss 0.43001\n",
      "STEP  431860 END @ 2017-03-28 07:47:05.043449, training logloss 0.40118, ave validation logloss 0.42958\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-433219 >> ave validation logloss 0.42852\n",
      "STEP  433370 END @ 2017-03-28 07:48:57.948425, training logloss 0.41702, ave validation logloss 0.42862\n",
      "STEP  434880 END @ 2017-03-28 07:50:47.981464, training logloss 0.42471, ave validation logloss 0.42974\n",
      "STEP  436390 END @ 2017-03-28 07:52:38.041305, training logloss 0.40456, ave validation logloss 0.42991\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-437145 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-437145 and pickle_nn_01 >>\n",
      "STEP  437900 END @ 2017-03-28 07:54:31.936705, training logloss 0.38997, ave validation logloss 0.42912\n",
      "STEP  439410 END @ 2017-03-28 07:56:22.682307, training logloss 0.40419, ave validation logloss 0.43018\n",
      "STEP  440920 END @ 2017-03-28 07:58:13.496412, training logloss 0.43508, ave validation logloss 0.42994\n",
      "STEP  442430 END @ 2017-03-28 08:00:04.141523, training logloss 0.40786, ave validation logloss 0.42948\n",
      "STEP  443940 END @ 2017-03-28 08:01:54.697372, training logloss 0.39803, ave validation logloss 0.42918\n",
      "STEP  445450 END @ 2017-03-28 08:03:45.300095, training logloss 0.39601, ave validation logloss 0.43076\n",
      "STEP  446960 END @ 2017-03-28 08:05:35.509250, training logloss 0.42205, ave validation logloss 0.42985\n",
      "STEP  448470 END @ 2017-03-28 08:07:25.933340, training logloss 0.41030, ave validation logloss 0.42966\n",
      "STEP  449980 END @ 2017-03-28 08:09:15.713890, training logloss 0.42386, ave validation logloss 0.43026\n",
      "STEP  451490 END @ 2017-03-28 08:11:05.650155, training logloss 0.41036, ave validation logloss 0.42950\n",
      "STEP  453000 END @ 2017-03-28 08:12:55.845776, training logloss 0.39409, ave validation logloss 0.42898\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-453151 >> ave validation logloss 0.42823\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-453302 >> ave validation logloss 0.42821\n",
      "STEP  454510 END @ 2017-03-28 08:14:50.240706, training logloss 0.42030, ave validation logloss 0.42873\n",
      "STEP  456020 END @ 2017-03-28 08:16:40.778526, training logloss 0.42571, ave validation logloss 0.42958\n",
      "STEP  457530 END @ 2017-03-28 08:18:30.549118, training logloss 0.38876, ave validation logloss 0.42907\n",
      "STEP  459040 END @ 2017-03-28 08:20:20.478290, training logloss 0.40089, ave validation logloss 0.43018\n",
      "STEP  460550 END @ 2017-03-28 08:22:10.699584, training logloss 0.38954, ave validation logloss 0.42870\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-461154 >> ave validation logloss 0.42811\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-461456 >> ave validation logloss 0.42783\n",
      "STEP  462060 END @ 2017-03-28 08:24:05.648943, training logloss 0.40713, ave validation logloss 0.42890\n",
      "STEP  463570 END @ 2017-03-28 08:25:56.702340, training logloss 0.42033, ave validation logloss 0.42816\n",
      "STEP  465080 END @ 2017-03-28 08:27:47.556637, training logloss 0.42665, ave validation logloss 0.42971\n",
      "STEP  466590 END @ 2017-03-28 08:29:38.048181, training logloss 0.41290, ave validation logloss 0.43013\n",
      "STEP  468100 END @ 2017-03-28 08:31:28.413243, training logloss 0.41622, ave validation logloss 0.42967\n",
      "STEP  469610 END @ 2017-03-28 08:33:18.732400, training logloss 0.40379, ave validation logloss 0.42983\n",
      "STEP  471120 END @ 2017-03-28 08:35:09.069513, training logloss 0.40959, ave validation logloss 0.42961\n",
      "STEP  472630 END @ 2017-03-28 08:36:59.104674, training logloss 0.39757, ave validation logloss 0.42974\n",
      "STEP  474140 END @ 2017-03-28 08:38:49.517242, training logloss 0.40470, ave validation logloss 0.43048\n",
      "STEP  475650 END @ 2017-03-28 08:40:39.770967, training logloss 0.39665, ave validation logloss 0.42907\n",
      "STEP  477160 END @ 2017-03-28 08:42:29.683005, training logloss 0.42403, ave validation logloss 0.42932\n",
      "STEP  478670 END @ 2017-03-28 08:44:19.484679, training logloss 0.40256, ave validation logloss 0.42795\n",
      "STEP  480180 END @ 2017-03-28 08:46:09.566734, training logloss 0.39965, ave validation logloss 0.42899\n",
      "STEP  481690 END @ 2017-03-28 08:47:59.457551, training logloss 0.40591, ave validation logloss 0.42875\n",
      "STEP  483200 END @ 2017-03-28 08:49:50.483165, training logloss 0.42099, ave validation logloss 0.42883\n",
      "STEP  484710 END @ 2017-03-28 08:51:40.993754, training logloss 0.42959, ave validation logloss 0.42924\n",
      "STEP  486220 END @ 2017-03-28 08:53:31.821019, training logloss 0.42181, ave validation logloss 0.43004\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-486371 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-486371 and pickle_nn_01 >>\n",
      "STEP  487730 END @ 2017-03-28 08:55:24.567436, training logloss 0.42028, ave validation logloss 0.42832\n",
      "STEP  489240 END @ 2017-03-28 08:57:15.028948, training logloss 0.42354, ave validation logloss 0.42978\n",
      "STEP  490750 END @ 2017-03-28 08:59:05.494781, training logloss 0.40911, ave validation logloss 0.42918\n",
      "STEP  492260 END @ 2017-03-28 09:00:55.722037, training logloss 0.41987, ave validation logloss 0.42942\n",
      "STEP  493770 END @ 2017-03-28 09:02:45.886333, training logloss 0.41208, ave validation logloss 0.42983\n",
      "STEP  495280 END @ 2017-03-28 09:04:36.100617, training logloss 0.42180, ave validation logloss 0.42914\n",
      "STEP  496790 END @ 2017-03-28 09:06:26.704625, training logloss 0.40635, ave validation logloss 0.42870\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-498300 >> ave validation logloss 0.42774\n",
      "STEP  498300 END @ 2017-03-28 09:08:18.952769, training logloss 0.39093, ave validation logloss 0.42774\n",
      "STEP  499810 END @ 2017-03-28 09:10:08.943602, training logloss 0.39065, ave validation logloss 0.42904\n",
      "STEP  501320 END @ 2017-03-28 09:11:59.160176, training logloss 0.40018, ave validation logloss 0.42844\n",
      "STEP  502830 END @ 2017-03-28 09:13:49.264787, training logloss 0.39706, ave validation logloss 0.43015\n",
      "STEP  504340 END @ 2017-03-28 09:15:38.852497, training logloss 0.41178, ave validation logloss 0.42896\n",
      "STEP  505850 END @ 2017-03-28 09:17:28.461894, training logloss 0.39882, ave validation logloss 0.42867\n",
      "STEP  507360 END @ 2017-03-28 09:19:19.143434, training logloss 0.40932, ave validation logloss 0.42922\n",
      "STEP  508870 END @ 2017-03-28 09:21:09.885893, training logloss 0.39947, ave validation logloss 0.42855\n",
      "STEP  510380 END @ 2017-03-28 09:23:00.718258, training logloss 0.40417, ave validation logloss 0.42920\n",
      "STEP  511890 END @ 2017-03-28 09:24:51.261446, training logloss 0.41245, ave validation logloss 0.42909\n",
      "STEP  513400 END @ 2017-03-28 09:26:41.865535, training logloss 0.39003, ave validation logloss 0.42875\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-514759 >> ave validation logloss 0.42743\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-514910 >> ave validation logloss 0.42738\n",
      "STEP  514910 END @ 2017-03-28 09:28:36.580909, training logloss 0.38839, ave validation logloss 0.42738\n",
      "STEP  516420 END @ 2017-03-28 09:30:27.043906, training logloss 0.39602, ave validation logloss 0.42791\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-516571 >> ave validation logloss 0.42730\n",
      "STEP  517930 END @ 2017-03-28 09:32:26.582820, training logloss 0.40652, ave validation logloss 0.42862\n",
      "STEP  519440 END @ 2017-03-28 09:34:16.952011, training logloss 0.41285, ave validation logloss 0.42874\n",
      "STEP  520950 END @ 2017-03-28 09:36:07.616941, training logloss 0.43394, ave validation logloss 0.42869\n",
      "STEP  522460 END @ 2017-03-28 09:37:57.666660, training logloss 0.41814, ave validation logloss 0.42919\n",
      "STEP  523970 END @ 2017-03-28 09:39:47.848248, training logloss 0.40864, ave validation logloss 0.42865\n",
      "STEP  525480 END @ 2017-03-28 09:41:37.880916, training logloss 0.41474, ave validation logloss 0.42890\n",
      "STEP  526990 END @ 2017-03-28 09:43:27.591831, training logloss 0.41161, ave validation logloss 0.42924\n",
      "STEP  528500 END @ 2017-03-28 09:45:17.715842, training logloss 0.40213, ave validation logloss 0.42893\n",
      "STEP  530010 END @ 2017-03-28 09:47:08.726467, training logloss 0.42860, ave validation logloss 0.42845\n",
      "STEP  531520 END @ 2017-03-28 09:48:59.748449, training logloss 0.42779, ave validation logloss 0.42853\n",
      "STEP  533030 END @ 2017-03-28 09:50:50.107403, training logloss 0.40797, ave validation logloss 0.42849\n",
      "STEP  534540 END @ 2017-03-28 09:52:40.643448, training logloss 0.42015, ave validation logloss 0.42953\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-535446 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-535446 and pickle_nn_01 >>\n",
      "STEP  536050 END @ 2017-03-28 09:54:33.406108, training logloss 0.40941, ave validation logloss 0.42855\n",
      "STEP  537560 END @ 2017-03-28 09:56:23.888826, training logloss 0.40757, ave validation logloss 0.42930\n",
      "STEP  539070 END @ 2017-03-28 09:58:14.298289, training logloss 0.40357, ave validation logloss 0.42788\n",
      "STEP  540580 END @ 2017-03-28 10:00:04.798190, training logloss 0.39772, ave validation logloss 0.42913\n",
      "STEP  542090 END @ 2017-03-28 10:01:55.149263, training logloss 0.42037, ave validation logloss 0.42786\n",
      "STEP  543600 END @ 2017-03-28 10:03:45.068612, training logloss 0.40848, ave validation logloss 0.42880\n",
      "STEP  545110 END @ 2017-03-28 10:05:35.457071, training logloss 0.40003, ave validation logloss 0.42825\n",
      "STEP  546620 END @ 2017-03-28 10:07:25.630252, training logloss 0.41867, ave validation logloss 0.42889\n",
      "STEP  548130 END @ 2017-03-28 10:09:15.193031, training logloss 0.41463, ave validation logloss 0.42789\n",
      "STEP  549640 END @ 2017-03-28 10:11:05.324465, training logloss 0.41609, ave validation logloss 0.42881\n",
      "STEP  551150 END @ 2017-03-28 10:12:54.996623, training logloss 0.41772, ave validation logloss 0.42944\n",
      "STEP  552660 END @ 2017-03-28 10:14:45.472201, training logloss 0.42013, ave validation logloss 0.42986\n",
      "STEP  554170 END @ 2017-03-28 10:16:36.430599, training logloss 0.40449, ave validation logloss 0.42885\n",
      "STEP  555680 END @ 2017-03-28 10:18:27.151411, training logloss 0.41996, ave validation logloss 0.42923\n",
      "STEP  557190 END @ 2017-03-28 10:20:17.695474, training logloss 0.43697, ave validation logloss 0.42888\n",
      "STEP  558700 END @ 2017-03-28 10:22:07.732586, training logloss 0.39604, ave validation logloss 0.42878\n",
      "STEP  560210 END @ 2017-03-28 10:23:58.119142, training logloss 0.39700, ave validation logloss 0.42805\n",
      "STEP  561720 END @ 2017-03-28 10:25:48.523038, training logloss 0.41347, ave validation logloss 0.42974\n",
      "STEP  563230 END @ 2017-03-28 10:27:39.075044, training logloss 0.41753, ave validation logloss 0.42915\n",
      "STEP  564740 END @ 2017-03-28 10:29:29.174053, training logloss 0.40951, ave validation logloss 0.42771\n",
      "STEP  566250 END @ 2017-03-28 10:31:19.498550, training logloss 0.42360, ave validation logloss 0.42862\n",
      "STEP  567760 END @ 2017-03-28 10:33:09.473320, training logloss 0.38672, ave validation logloss 0.42764\n",
      "STEP  569270 END @ 2017-03-28 10:34:59.852961, training logloss 0.41068, ave validation logloss 0.42838\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-570629 >> ave validation logloss 0.42727\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-570780 >> ave validation logloss 0.42716\n",
      "STEP  570780 END @ 2017-03-28 10:36:54.484354, training logloss 0.39699, ave validation logloss 0.42716\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-571082 >> ave validation logloss 0.42698\n",
      "STEP  572290 END @ 2017-03-28 10:38:47.092837, training logloss 0.41597, ave validation logloss 0.42864\n",
      "STEP  573800 END @ 2017-03-28 10:40:37.169103, training logloss 0.40065, ave validation logloss 0.42845\n",
      "STEP  575310 END @ 2017-03-28 10:42:27.459069, training logloss 0.43271, ave validation logloss 0.42859\n",
      "STEP  576820 END @ 2017-03-28 10:44:18.222884, training logloss 0.39756, ave validation logloss 0.42774\n",
      "STEP  578330 END @ 2017-03-28 10:46:08.550773, training logloss 0.38308, ave validation logloss 0.42750\n",
      "STEP  579840 END @ 2017-03-28 10:47:59.196186, training logloss 0.41339, ave validation logloss 0.42874\n",
      "STEP  581350 END @ 2017-03-28 10:49:49.938331, training logloss 0.42644, ave validation logloss 0.42817\n",
      "STEP  582860 END @ 2017-03-28 10:51:40.422638, training logloss 0.39998, ave validation logloss 0.42884\n",
      "STEP  584370 END @ 2017-03-28 10:53:30.845596, training logloss 0.42107, ave validation logloss 0.42822\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-584672 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-584672 and pickle_nn_01 >>\n",
      "STEP  585880 END @ 2017-03-28 10:55:23.367743, training logloss 0.41846, ave validation logloss 0.42881\n",
      "STEP  587390 END @ 2017-03-28 10:57:13.344955, training logloss 0.42803, ave validation logloss 0.42949\n",
      "STEP  588900 END @ 2017-03-28 10:59:03.366093, training logloss 0.40298, ave validation logloss 0.42906\n",
      "STEP  590410 END @ 2017-03-28 11:00:53.644315, training logloss 0.39342, ave validation logloss 0.42825\n",
      "STEP  591920 END @ 2017-03-28 11:02:43.569302, training logloss 0.42076, ave validation logloss 0.42852\n",
      "STEP  593430 END @ 2017-03-28 11:04:34.045299, training logloss 0.42645, ave validation logloss 0.42825\n",
      "STEP  594940 END @ 2017-03-28 11:06:24.229294, training logloss 0.39410, ave validation logloss 0.42848\n",
      "STEP  596450 END @ 2017-03-28 11:08:14.519761, training logloss 0.39761, ave validation logloss 0.42754\n",
      "STEP  597960 END @ 2017-03-28 11:10:04.430982, training logloss 0.41091, ave validation logloss 0.42767\n",
      "STEP  599470 END @ 2017-03-28 11:11:55.288140, training logloss 0.40080, ave validation logloss 0.42806\n",
      "STEP  600980 END @ 2017-03-28 11:13:45.818068, training logloss 0.39111, ave validation logloss 0.42831\n",
      "STEP  602490 END @ 2017-03-28 11:15:36.359145, training logloss 0.42317, ave validation logloss 0.42744\n",
      "STEP  604000 END @ 2017-03-28 11:17:26.860441, training logloss 0.42228, ave validation logloss 0.42826\n",
      "STEP  605510 END @ 2017-03-28 11:19:17.124389, training logloss 0.40133, ave validation logloss 0.42831\n",
      "STEP  607020 END @ 2017-03-28 11:21:07.979897, training logloss 0.44009, ave validation logloss 0.42846\n",
      "STEP  608530 END @ 2017-03-28 11:22:58.453953, training logloss 0.43248, ave validation logloss 0.42835\n",
      "STEP  610040 END @ 2017-03-28 11:24:48.877192, training logloss 0.41835, ave validation logloss 0.42791\n",
      "STEP  611550 END @ 2017-03-28 11:26:38.968309, training logloss 0.41905, ave validation logloss 0.42912\n",
      "STEP  613060 END @ 2017-03-28 11:28:28.980484, training logloss 0.42365, ave validation logloss 0.42944\n",
      "STEP  614570 END @ 2017-03-28 11:30:19.188417, training logloss 0.41124, ave validation logloss 0.42701\n",
      "STEP  616080 END @ 2017-03-28 11:32:09.616893, training logloss 0.42428, ave validation logloss 0.42825\n",
      "STEP  617590 END @ 2017-03-28 11:33:59.879268, training logloss 0.41047, ave validation logloss 0.42763\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-618647 >> ave validation logloss 0.42643\n",
      "STEP  619100 END @ 2017-03-28 11:35:51.728567, training logloss 0.41067, ave validation logloss 0.42689\n",
      "STEP  620610 END @ 2017-03-28 11:37:42.086007, training logloss 0.40046, ave validation logloss 0.42826\n",
      "STEP  622120 END @ 2017-03-28 11:39:32.577502, training logloss 0.40157, ave validation logloss 0.42857\n",
      "STEP  623630 END @ 2017-03-28 11:41:23.740444, training logloss 0.40631, ave validation logloss 0.42884\n",
      "STEP  625140 END @ 2017-03-28 11:43:14.556150, training logloss 0.40568, ave validation logloss 0.42888\n",
      "STEP  626650 END @ 2017-03-28 11:45:05.112004, training logloss 0.41853, ave validation logloss 0.42908\n",
      "STEP  628160 END @ 2017-03-28 11:46:56.053771, training logloss 0.42593, ave validation logloss 0.42869\n",
      "STEP  629670 END @ 2017-03-28 11:48:46.312843, training logloss 0.40144, ave validation logloss 0.42768\n",
      "STEP  631180 END @ 2017-03-28 11:50:36.914362, training logloss 0.40693, ave validation logloss 0.42893\n",
      "STEP  632690 END @ 2017-03-28 11:52:27.478866, training logloss 0.39349, ave validation logloss 0.42681\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-633898 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-633898 and pickle_nn_01 >>\n",
      "STEP  634200 END @ 2017-03-28 11:54:20.009735, training logloss 0.39992, ave validation logloss 0.42827\n",
      "STEP  635710 END @ 2017-03-28 11:56:10.114483, training logloss 0.42646, ave validation logloss 0.42798\n",
      "STEP  637220 END @ 2017-03-28 11:58:00.403785, training logloss 0.39682, ave validation logloss 0.42805\n",
      "STEP  638730 END @ 2017-03-28 11:59:50.083265, training logloss 0.38574, ave validation logloss 0.42822\n",
      "STEP  640240 END @ 2017-03-28 12:01:40.272971, training logloss 0.41855, ave validation logloss 0.42926\n",
      "STEP  641750 END @ 2017-03-28 12:03:30.318910, training logloss 0.39352, ave validation logloss 0.42811\n",
      "STEP  643260 END @ 2017-03-28 12:05:19.837617, training logloss 0.38916, ave validation logloss 0.42654\n",
      "STEP  644770 END @ 2017-03-28 12:07:10.803441, training logloss 0.39079, ave validation logloss 0.42785\n",
      "STEP  646280 END @ 2017-03-28 12:09:01.882399, training logloss 0.41011, ave validation logloss 0.42776\n",
      "STEP  647790 END @ 2017-03-28 12:10:52.336322, training logloss 0.40842, ave validation logloss 0.42752\n",
      "STEP  649300 END @ 2017-03-28 12:12:42.853064, training logloss 0.39593, ave validation logloss 0.42683\n",
      "STEP  650810 END @ 2017-03-28 12:14:33.169225, training logloss 0.41791, ave validation logloss 0.42845\n",
      "STEP  652320 END @ 2017-03-28 12:16:23.812449, training logloss 0.43905, ave validation logloss 0.42751\n",
      "STEP  653830 END @ 2017-03-28 12:18:14.517538, training logloss 0.40120, ave validation logloss 0.43008\n",
      "STEP  655340 END @ 2017-03-28 12:20:05.033205, training logloss 0.41955, ave validation logloss 0.42783\n",
      "STEP  656850 END @ 2017-03-28 12:21:55.271690, training logloss 0.43071, ave validation logloss 0.42889\n",
      "STEP  658360 END @ 2017-03-28 12:23:45.446043, training logloss 0.43439, ave validation logloss 0.42876\n",
      "STEP  659870 END @ 2017-03-28 12:25:35.592471, training logloss 0.41446, ave validation logloss 0.42761\n",
      "STEP  661380 END @ 2017-03-28 12:27:25.741246, training logloss 0.42074, ave validation logloss 0.42825\n",
      "STEP  662890 END @ 2017-03-28 12:29:15.730206, training logloss 0.41681, ave validation logloss 0.42842\n",
      "STEP  664400 END @ 2017-03-28 12:31:05.792214, training logloss 0.40286, ave validation logloss 0.42905\n",
      "STEP  665910 END @ 2017-03-28 12:32:56.081498, training logloss 0.43780, ave validation logloss 0.42802\n",
      "STEP  667420 END @ 2017-03-28 12:34:46.849489, training logloss 0.40229, ave validation logloss 0.42797\n",
      "STEP  668930 END @ 2017-03-28 12:36:37.942163, training logloss 0.39756, ave validation logloss 0.42845\n",
      "STEP  670440 END @ 2017-03-28 12:38:28.601409, training logloss 0.42141, ave validation logloss 0.42753\n",
      "STEP  671950 END @ 2017-03-28 12:40:19.142870, training logloss 0.40066, ave validation logloss 0.42889\n",
      "STEP  673460 END @ 2017-03-28 12:42:09.839760, training logloss 0.40806, ave validation logloss 0.42890\n",
      "STEP  674970 END @ 2017-03-28 12:44:00.148891, training logloss 0.42230, ave validation logloss 0.42844\n",
      "STEP  676480 END @ 2017-03-28 12:45:50.455572, training logloss 0.40066, ave validation logloss 0.42802\n",
      "STEP  677990 END @ 2017-03-28 12:47:41.117013, training logloss 0.42327, ave validation logloss 0.42909\n",
      "STEP  679500 END @ 2017-03-28 12:49:31.584805, training logloss 0.43823, ave validation logloss 0.42787\n",
      "STEP  681010 END @ 2017-03-28 12:51:21.613900, training logloss 0.40377, ave validation logloss 0.42788\n",
      "STEP  682520 END @ 2017-03-28 12:53:11.475709, training logloss 0.43577, ave validation logloss 0.42790\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-683275 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-683275 and pickle_nn_01 >>\n",
      "STEP  684030 END @ 2017-03-28 12:55:03.779691, training logloss 0.39393, ave validation logloss 0.42765\n",
      "STEP  685540 END @ 2017-03-28 12:56:53.936671, training logloss 0.41397, ave validation logloss 0.42808\n",
      "STEP  687050 END @ 2017-03-28 12:58:44.030600, training logloss 0.39550, ave validation logloss 0.42855\n",
      "STEP  688560 END @ 2017-03-28 13:00:34.003679, training logloss 0.41185, ave validation logloss 0.42741\n",
      "STEP  690070 END @ 2017-03-28 13:02:24.481826, training logloss 0.42908, ave validation logloss 0.42714\n",
      "STEP  691580 END @ 2017-03-28 13:04:15.318103, training logloss 0.42028, ave validation logloss 0.42822\n",
      "STEP  693090 END @ 2017-03-28 13:06:05.725749, training logloss 0.39692, ave validation logloss 0.42885\n",
      "STEP  694600 END @ 2017-03-28 13:07:56.477012, training logloss 0.40874, ave validation logloss 0.42778\n",
      "STEP  696110 END @ 2017-03-28 13:09:46.987534, training logloss 0.42869, ave validation logloss 0.42831\n",
      "STEP  697620 END @ 2017-03-28 13:11:37.638425, training logloss 0.42199, ave validation logloss 0.42784\n",
      "STEP  699130 END @ 2017-03-28 13:13:28.183596, training logloss 0.41593, ave validation logloss 0.42772\n",
      "STEP  700640 END @ 2017-03-28 13:15:18.608480, training logloss 0.39280, ave validation logloss 0.42881\n",
      "STEP  702150 END @ 2017-03-28 13:17:09.041498, training logloss 0.41572, ave validation logloss 0.42917\n",
      "STEP  703660 END @ 2017-03-28 13:18:59.325327, training logloss 0.40342, ave validation logloss 0.42871\n",
      "STEP  705170 END @ 2017-03-28 13:20:49.414410, training logloss 0.43961, ave validation logloss 0.42773\n",
      "STEP  706680 END @ 2017-03-28 13:22:39.195679, training logloss 0.37564, ave validation logloss 0.42774\n",
      "STEP  708190 END @ 2017-03-28 13:24:29.306570, training logloss 0.40824, ave validation logloss 0.42814\n",
      "STEP  709700 END @ 2017-03-28 13:26:19.236924, training logloss 0.41904, ave validation logloss 0.42788\n",
      "STEP  711210 END @ 2017-03-28 13:28:09.550359, training logloss 0.39061, ave validation logloss 0.42897\n",
      "STEP  712720 END @ 2017-03-28 13:29:59.707966, training logloss 0.39928, ave validation logloss 0.42745\n",
      "STEP  714230 END @ 2017-03-28 13:31:50.370719, training logloss 0.42069, ave validation logloss 0.42746\n",
      "STEP  715740 END @ 2017-03-28 13:33:40.893258, training logloss 0.40485, ave validation logloss 0.42853\n",
      "STEP  717250 END @ 2017-03-28 13:35:31.728112, training logloss 0.41251, ave validation logloss 0.42854\n",
      "STEP  718760 END @ 2017-03-28 13:37:22.386800, training logloss 0.40550, ave validation logloss 0.42883\n",
      "STEP  720270 END @ 2017-03-28 13:39:12.543294, training logloss 0.44180, ave validation logloss 0.42873\n",
      "STEP  721780 END @ 2017-03-28 13:41:02.721347, training logloss 0.40080, ave validation logloss 0.42894\n",
      "STEP  723290 END @ 2017-03-28 13:42:53.149653, training logloss 0.43899, ave validation logloss 0.42785\n",
      "STEP  724800 END @ 2017-03-28 13:44:43.431622, training logloss 0.40680, ave validation logloss 0.42748\n",
      "STEP  726310 END @ 2017-03-28 13:46:33.163165, training logloss 0.42731, ave validation logloss 0.42781\n",
      "STEP  727820 END @ 2017-03-28 13:48:23.469406, training logloss 0.42060, ave validation logloss 0.42727\n",
      "STEP  729330 END @ 2017-03-28 13:50:13.263362, training logloss 0.40257, ave validation logloss 0.42711\n",
      "STEP  730840 END @ 2017-03-28 13:52:03.171806, training logloss 0.40210, ave validation logloss 0.42759\n",
      "STEP  732350 END @ 2017-03-28 13:53:53.357680, training logloss 0.38223, ave validation logloss 0.42750\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-732652 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-732652 and pickle_nn_01 >>\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-733407 >> ave validation logloss 0.42636\n",
      "STEP  733860 END @ 2017-03-28 13:55:54.884028, training logloss 0.41382, ave validation logloss 0.42699\n",
      "STEP  735370 END @ 2017-03-28 13:57:44.752129, training logloss 0.39168, ave validation logloss 0.42791\n",
      "STEP  736880 END @ 2017-03-28 13:59:35.709106, training logloss 0.42789, ave validation logloss 0.42861\n",
      "STEP  738390 END @ 2017-03-28 14:01:26.619484, training logloss 0.38751, ave validation logloss 0.42834\n",
      "STEP  739900 END @ 2017-03-28 14:03:17.530702, training logloss 0.42925, ave validation logloss 0.42730\n",
      "STEP  741410 END @ 2017-03-28 14:05:07.956572, training logloss 0.38729, ave validation logloss 0.42729\n",
      "STEP  742920 END @ 2017-03-28 14:06:58.455225, training logloss 0.39377, ave validation logloss 0.42729\n",
      "STEP  744430 END @ 2017-03-28 14:08:48.901033, training logloss 0.40116, ave validation logloss 0.42817\n",
      "STEP  745940 END @ 2017-03-28 14:10:39.228432, training logloss 0.39472, ave validation logloss 0.42777\n",
      "STEP  747450 END @ 2017-03-28 14:12:29.689335, training logloss 0.41962, ave validation logloss 0.42729\n",
      "STEP  748960 END @ 2017-03-28 14:14:20.042599, training logloss 0.41168, ave validation logloss 0.42750\n",
      "STEP  750470 END @ 2017-03-28 14:16:10.255263, training logloss 0.39835, ave validation logloss 0.42773\n",
      "STEP  751980 END @ 2017-03-28 14:18:00.243715, training logloss 0.38343, ave validation logloss 0.42761\n",
      "STEP  753490 END @ 2017-03-28 14:19:50.517813, training logloss 0.39232, ave validation logloss 0.42792\n",
      "STEP  755000 END @ 2017-03-28 14:21:40.301701, training logloss 0.41836, ave validation logloss 0.42744\n",
      "STEP  756510 END @ 2017-03-28 14:23:30.405251, training logloss 0.42158, ave validation logloss 0.42802\n",
      "STEP  758020 END @ 2017-03-28 14:25:20.418114, training logloss 0.39854, ave validation logloss 0.42786\n",
      "STEP  759530 END @ 2017-03-28 14:27:11.032740, training logloss 0.42362, ave validation logloss 0.42815\n",
      "STEP  761040 END @ 2017-03-28 14:29:01.650496, training logloss 0.39015, ave validation logloss 0.42760\n",
      "STEP  762550 END @ 2017-03-28 14:30:52.305310, training logloss 0.40853, ave validation logloss 0.42798\n",
      "STEP  764060 END @ 2017-03-28 14:32:42.923217, training logloss 0.39207, ave validation logloss 0.42859\n",
      "STEP  765570 END @ 2017-03-28 14:34:33.407976, training logloss 0.39698, ave validation logloss 0.42845\n",
      "STEP  767080 END @ 2017-03-28 14:36:23.814097, training logloss 0.41479, ave validation logloss 0.42653\n",
      "STEP  768590 END @ 2017-03-28 14:38:14.463818, training logloss 0.39587, ave validation logloss 0.42791\n",
      "STEP  770100 END @ 2017-03-28 14:40:04.740466, training logloss 0.41606, ave validation logloss 0.42695\n",
      "STEP  771610 END @ 2017-03-28 14:41:54.758172, training logloss 0.40580, ave validation logloss 0.42706\n",
      "STEP  773120 END @ 2017-03-28 14:43:44.734904, training logloss 0.43692, ave validation logloss 0.42785\n",
      "STEP  774630 END @ 2017-03-28 14:45:34.656824, training logloss 0.38881, ave validation logloss 0.42736\n",
      "STEP  776140 END @ 2017-03-28 14:47:24.695927, training logloss 0.41262, ave validation logloss 0.42845\n",
      "STEP  777650 END @ 2017-03-28 14:49:14.916207, training logloss 0.38979, ave validation logloss 0.42708\n",
      "STEP  779160 END @ 2017-03-28 14:51:04.619337, training logloss 0.41081, ave validation logloss 0.42759\n",
      "STEP  780670 END @ 2017-03-28 14:52:54.242254, training logloss 0.43860, ave validation logloss 0.42747\n",
      "INFO:tensorflow:ckpt_nn_01/hourly/model-781878 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "<< hourly save at ckpt_nn_01/hourly/model-781878 and pickle_nn_01 >>\n",
      "STEP  782180 END @ 2017-03-28 14:54:47.117908, training logloss 0.41065, ave validation logloss 0.42865\n",
      "STEP  783690 END @ 2017-03-28 14:56:38.149789, training logloss 0.41854, ave validation logloss 0.42681\n",
      "STEP  785200 END @ 2017-03-28 14:58:29.085294, training logloss 0.40790, ave validation logloss 0.42798\n",
      "STEP  786710 END @ 2017-03-28 15:00:19.886619, training logloss 0.41771, ave validation logloss 0.42785\n",
      "STEP  788220 END @ 2017-03-28 15:02:10.735927, training logloss 0.40017, ave validation logloss 0.42763\n",
      "STEP  789730 END @ 2017-03-28 15:04:00.885943, training logloss 0.39886, ave validation logloss 0.42714\n",
      "STEP  791240 END @ 2017-03-28 15:05:51.357427, training logloss 0.38895, ave validation logloss 0.42756\n",
      "STEP  792750 END @ 2017-03-28 15:07:41.773364, training logloss 0.39044, ave validation logloss 0.42689\n",
      "STEP  794260 END @ 2017-03-28 15:09:31.925715, training logloss 0.40412, ave validation logloss 0.42708\n",
      "STEP  795770 END @ 2017-03-28 15:11:22.221013, training logloss 0.41475, ave validation logloss 0.42677\n",
      "STEP  797280 END @ 2017-03-28 15:13:12.251754, training logloss 0.39831, ave validation logloss 0.42781\n",
      "STEP  798790 END @ 2017-03-28 15:15:01.996738, training logloss 0.42149, ave validation logloss 0.42687\n",
      "STEP  800300 END @ 2017-03-28 15:16:51.830649, training logloss 0.41578, ave validation logloss 0.42764\n",
      "STEP  801810 END @ 2017-03-28 15:18:41.979193, training logloss 0.37937, ave validation logloss 0.42760\n",
      "STEP  803320 END @ 2017-03-28 15:20:31.988887, training logloss 0.39727, ave validation logloss 0.42734\n",
      "STEP  804830 END @ 2017-03-28 15:22:22.142521, training logloss 0.40222, ave validation logloss 0.42714\n",
      "STEP  806340 END @ 2017-03-28 15:24:12.745408, training logloss 0.41473, ave validation logloss 0.42678\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-807397 >> ave validation logloss 0.42626\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-807548 >> ave validation logloss 0.42624\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-807699 >> ave validation logloss 0.42602\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-807850 >> ave validation logloss 0.42593\n",
      "STEP  807850 END @ 2017-03-28 15:26:12.551962, training logloss 0.39998, ave validation logloss 0.42593\n",
      "STEP  809360 END @ 2017-03-28 15:28:04.056118, training logloss 0.39243, ave validation logloss 0.42608\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-809662 >> ave validation logloss 0.42589\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-809813 >> ave validation logloss 0.42587\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-809964 >> ave validation logloss 0.42567\n",
      "<< best model so far is saved in ckpt_nn_01/best/model-810115 >> ave validation logloss 0.42563\n",
      "STEP  810870 END @ 2017-03-28 15:30:04.417080, training logloss 0.41323, ave validation logloss 0.42641\n",
      "STEP  812380 END @ 2017-03-28 15:31:55.220082, training logloss 0.42451, ave validation logloss 0.42756\n",
      "STEP  813890 END @ 2017-03-28 15:33:45.615997, training logloss 0.40484, ave validation logloss 0.42706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cf06d6cf6561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogloss_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/kumiko/ML/Kaggle Quora/helper_functions.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(graph_variables, logloss_log, train_data, train_labels, validation_data, validation_labels)\u001b[0m\n\u001b[1;32m     84\u001b[0m             _, summary = sess.run([graph_variables['optimizer'], graph_variables['summarizer']], \n\u001b[1;32m     85\u001b[0m                                   feed_dict={graph_variables['data']: batch_data, graph_variables['labels']: batch_labels,\n\u001b[0;32m---> 86\u001b[0;31m                                              graph_variables['keep_prob']: logloss_log['variables']['dropout_train_keep_prob'], graph_variables['is_training']: True})\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlogloss_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'variables'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_every'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kumiko/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kumiko/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kumiko/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kumiko/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kumiko/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(graph_variables, logloss_log, train_X, train_y, validation_X, validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y_nn_01 = predict(graph_variables, logloss_log, test_X)\n",
    "pred_nn_01 = pd.merge(test, pd.DataFrame(test_y_nn_01).rename(columns={0: 'is_duplicate'}), left_index=True, right_index=True)\n",
    "pred_nn_01[['test_id', 'is_duplicate']].to_csv('submit_16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA80AAACZCAYAAAAVQdiJAAAKGWlDQ1BJQ0MgUHJvZmlsZQAASImV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\nJsAEmEBNE/h/Iem/DsHMme0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='Screen Shot 2017-03-28 at 3.39.32 PM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
